# config-default.yaml: LibKGE's default configuration options

# Control output is printed to the console.
console:
  # If set, no console output is produced
  quiet: False
  folder: "/mnt/data1/ma/gengyuan/experiment/logging/desimple"


  # Formatting of trace entries folsr console output after certain events, such as
  # finishing an epoch. Each entry is a key-value pair, where the key refers to
  # the type of event and the value is a Python expression (which may access
  # variables "trace" and "config" as well as all trace entries). If no
  # expression for a given key is specified or if the expression is empty, the
  # full trace is produced.
  #
  # Supported keys: train_epoch, eval_epoch
  #
  # Example (one liners for train and eval):
  # format:
  #   train_epoch: 'f"{config.get(''train.type'')}-{config.get(''train.loss'')}: epoch={epoch:3d} avg_cost={avg_cost:.4E} avg_loss={avg_loss:.4E} avg_pens={sum(avg_penalties.values()):.4E} time={epoch_time:.2f}"'
  #   eval_epoch: 'f"{config.get(''eval.type'')}: epoch={epoch:3d} {config.get(''valid.metric'')}={trace[config.get(''valid.metric'')]:.4E} time={epoch_time:.2f}"'
  format: {}


# Job-level configuration options.
task:
  # Type of job to run. Possible values are "train", "eval", and "search". See
  # the corresponding configuration keys for mode information.
  type: train

  # Main device to use for this job (e.g., 'cpu', 'cuda', 'cuda:0')
  device: 'cuda'

  reciprocal_relation: False


# The seeds of the PRNGs can be set manually for (increased) reproducibility.
# Use -1 to disable explicit seeding.
random_seed:
  # Seed used to initialize each of the PRNGs below in case they do not have
  # their own seed specified. The actual seed used computed from the value given
  # here and the name of the PRNG (e.g., python, torch, ...). If -1, disable
  # default seeding.
  default: -1

  python: -1
  torch: -1
  numpy: -1
  numba: -1


## DATASET #####################################################################

dataset:
  # data folder includes train, val, test files
#  folder: '/Users/GengyuanMax/workspace/tkge/data/icews14'
  folder: /mnt/data1/ma/gengyuan/tkge/data/icews14

  # indexes mapping [false, true]
  # TODO 可以保存到cache
  mapping: False
  filter: False
  temporal:
    resolution: "day"
    index: False
    float: True
  args: ~



  # Specify a data here. There must be a folder of that name under "data/".
  # If this folder contains a data.yaml file, it overrides the defaults
  # specified below.
  name: 'icews14'

  # Number of entities. If set to -1, automatically determined from the
  # entity_ids file (see below).
  num_entities: -1

  # Number of relations. If set to -1, automatically determined from the
  # relation_ids file (see below).
  num_relations: -1

    # A list of files associated with this data, each associated with a key.
  # Each entry must contain at least the filename and the type fields.


  # Files that store human-readble string representations of each
  # entity/relation. These files are optional.
  #
  # Type can be map (field 0 is internal index) or idmap (field 0 is external
  # id).


  # Additional files can be added as needed
  +++: +++

  # Whether to store processed data files and indexes as binary files in the
  # data directory. This enables faster loading at the cost of storage space.
  # LibKGE will ensure that pickled files are only used when not outdated. Note
  # that the value specified here may be overwritten by data-specific choices
  # (in data.yaml).
  pickle: True

  # Additional data specific keys can be added as needed


## MODEL #######################################################################

# Which KGE model to use. Examples include "rescal", "complex", "distmult",
# "transe", or "conve". For available models, see the project homepage and/or
# configuration files under kge/model.
model:
  name: 'de_simple'
  embedding:
    emb_dim: 100
    se_prop: 0.68
  dropout: 0.4
  args: ~


negative_sampling:
  name: 'time_agnostic'
  num_samples: 500
  filter: False
  as_matrix: True
  target: both


## TRAINING ####################################################################

# Options of training jobs (job.type=="train")
train:
  # Split used for training (specified under 'data.files').
  split: train
  type: negative_sampling

  loss:
    type: cross_entropy_loss
  max_epochs: 500

  loader:
    num_workers: 0
    pin_memory: False
    drop_last: False
    timeout: 0

  valid:
    split: test # in [test or valid]
    every: 10
    batch_size: 100
    filter: time-aware  # in [off, static, time-aware]
    ordering: optimistic    # in [optimistic, peesimistic]
    k: [1, 3, 10]

  batch_size: 512
  subbatch_size: -1
  subbatch_auto_tune: False
  optimizer:
    type: Adam
    ars:
      lr: 0.001
      reg_lambda: 0.0

    default:
      type: Adam           # sgd, adagrad, adam

      args:
        +++: +++

  regularizer: ~
  inplace_regularizer: ~

  # Learning rate scheduler to use. Any scheduler from torch.optim.lr_scheduler
  # can be used (e.g., ReduceLROnPlateau). When left empty, no LR scheduler is
  # used.
  lr_scheduler: ""

  # Additional arguments for the scheduler.
  lr_scheduler_args:
    +++: +++

  # When to write entries to the trace file.
  trace_level: epoch           # batch, epoch

  # When to create checkpoints
  checkpoint:
    # In addition the the checkpoint of the last epoch (which is transient),
    # create an additional checkpoint every this many epochs. Disable additional
    # checkpoints with 0.
    folder: /mnt/data1/ma/gengyuan/experiment/ckpt
    every: 1

    # Keep this many most recent additional checkpoints.
    keep: 3

  # When set, LibKGE automatically corrects certain invalid configuration
  # options. Each such change is logged. When not set and the configuration is
  # invalid, LibKGE raises an error.
  auto_correct: False

  # Abort training (with an error) when the value of the cost function becomes
  # not a number.
  abort_on_nan: True

  # If set, create a PDF of the compute graph (of first batch of first epoch).
  visualize_graph: False

  # Other options


eval:
  filter: time-aware
  ordering: descending
  preference: optimistic
  k: [1,3,10]


test:
  batch_size: 256
  loader:
    num_workers: 0
    pin_memory: False
    drop_last: False
    timeout: 0
  model_path: +++ # TODO




# Options for KvsAll training (train.type=="KvsAll")
KvsAll:
  # Amount of label smoothing (disabled when 0) for sp_ and _po queries.
  # Disencourages models to perform extreme predictions (0 or 1).
  #
  # Technically, reduces all labels by fraction given by this value and
  # subsequently increases them by 1.0/num_entities. For example, 0s become
  # 1.0/num_entities and 1s become (1.0-label_smoothing)+1.0/num_entities.
  #
  # This form of label smoothing was used by ConvE
  # ( https://github.com/TimDettmers/ConvE/blob/853d7010a4b569c3d24c9e6eeaf9dc3535d78832/main.py#L156) with a default value of 0.1.
  label_smoothing: 0.0

  # Query types used during training. Here _ indicates the prediction target.
  # For example, sp_ means queries of form (s,p,?): predict all objects for each
  # distinct subject-predicate pair (s,p).
  query_types:
    sp_: True
    s_o: False
    _po: True

## Options for negative sampling training (train.type=="negative_sampling")
#negative_sampling:
#  # Negative sampler to use
#  # - uniform  : samples entities/relations for corruption uniformly from the set
#  #              of entities/relations
#  # - frequency: samples entities/relations for corruption based on their relative
#  #              frequency in the corresponding slot in the training data
#  sampling_type: uniform
#
#  # Options for sampling type frequency (negative_sampling.sampling_type=="frequency")
#  frequency:
#    # Smoothing constant to add to frequencies in training data (disable with
#    # 0). Corresponds to a symmetric Dirichlet prior.
#    smoothing: 1
#
#  # Number of times each slot of each positive triple is corrupted by the
#  # sampler to obtain negative triples.
#  num_samples:
#    s: 3
#    p: 0          # -1 means: same as s
#    o: -1         # -1 means: same as s
#
#  # Whether to resample corrupted triples that occur in the training data (and
#  # are hence positives). Can be set separately for each slot.
#  filtering:
#    s: False       # filter and resample for slot s
#    p: False       # as above
#    o: False       # as above
#
#    split: ''      # split containing the positives; default is train.split
#
#    # Implementation to use for filtering.
#    # standard: use slow generic implementation, available for all samplers
#    # fast: use specialized fast implementation, available for some samplers
#    # fast_if_available: use fast implementation if available, else standard
#    implementation: fast_if_available
#
#  # Whether to share the s/p/o corruptions for all triples in the batch. This
#  # can make training more efficient. Cannot be used with together with
#  # filtering, but it is ensured that each triple does not get itself as a
#  # negative.
#  shared: False
#
#  # Type of shared sampling implementation:
#  #
#  # - naive: Each positive triples uses exactly the same set of shared samples,
#  #          even if it's own positive entity is in the shared sample as a
#  #          negative. Very fast.
#  # - default: Avoid that a positive triple's entity occurs in its shared
#  #            negative samples. This is done by using a shared sample size that
#  #            is one sample larger then needed. The actual sample of a triple
#  #            is then obtained by dropping the sample corresponding to its
#  #            positive entity (or a random one if there is no such sample).
#  shared_type: 'default'
#
#  # Whether sampling should be performed with replacement. Without replacement
#  # sampling is currently only supported for shared sampling.
#  with_replacement: True
#
#  # Implementation to use for the negative sampling job. Possible values are:
#  # - triple: Scores every positive and negative triple in the batch
#  # - all   : Scores against all possible targets and filters relevant scores
#  #           out of the resulting score matrix
#  # - batch : Scores against all targets contained in batch (or chunk) and
#  #           filters relevant scores out of the resulting score matrix.
#  # - auto  : Chooses best implementation based on num_samples.
#  #           'batch' if shared sampling is activated or max(num_samples.s,
#  #           num_samples.p, num_samples.o) > 30. 'triple' otherwise.
#  #
#  # 'batch' or 'auto' is recommended (faster) for models which have efficient
#  # implementations to score many targets at once. For all other models, use
#  # 'triple' (e.g., for TransE or RotatE in the current implementation).
#  implementation: triple

## VALIDATION AND EVALUATION ###################################################

# Options used for all evaluation jobs (job.type=="eval"). Also used during
# validation when training (unless overridden, see below). Right now, evaluation
# jobs compute a fixed set of metrics: MRR, HITS@k.
evaluation:
  # Split used for evaluation (specified under 'data.files').
  split: valid

  # Type of evaluation (entity_ranking, training_loss)
  type: entity_ranking

  # Batch size used during evaluation
  batch_size: 100

  # Amount of tracing information being written. When set to "example", traces
  # the rank of the correct answer for each example.
  trace_level: epoch            # example, batch, epoch

  # Number of workers used to construct batches. Leave at 0 for default (no
  # separate workers). Ofn Windows, only 0 is supported.
  num_workers: 0

  # Other options
  pin_memory: False


# Configuration options for model validation/selection during training. Applied
# in addition to the options set under "eval" above.
valid:
  # Split used for validation. If '', use eval.split, else override eval.split
  # during validation with this value.
  split: 'valid'

  # Validation is run every this many epochs during training (disable validation
  # with 0).
  every: 5

  # Name of the trace entry that holds the validation metric (higher value is
  # better). To add a custom metric, set this to a fresh name and define
  # metric_expr below.
  metric: mean_reciprocal_rank_filtered_with_test

  # If the above metric is not present in trace (e.g., because a custom metric
  # should be used), a Python expression to compute the metric. Can refer to
  # trace entries directly and to configuration options via config.
  #
  # Example: 'math.sqrt(mean_reciprocal_rank) + config.get("user.par")'
  metric_expr: 'float("nan")'

  # Whether the metric should be maximized (True, large value is better) or
  # minimized (False, small value is better). Affects things such as early
  # abort, learning rate scheduling, or hyperparameter search.
  metric_max: True

  early_stopping:
    # Grace period of validation runs before a training run is stopped early
    # (disable early stopping with 0). If the value is set to n, then training is
    # stopped when there has been no improvement (compared to the best overall
    # result so far) in the validation metric during the last n validation runs.
    patience: 5

    # A target validation metric value that should be reached after n epochs,
    # set to 0 epoch to turn off. Should be set very very conservatively and the
    # main purpose is for pruning completely useless hyperparameter settings
    # during hyper-parameter optimization.
    threshold:
      epochs: 0
      metric_value: 0.0

  # Amount of tracing information being written. When set to "example", traces
  # the rank of the correct answer for each example.
  trace_level: epoch

# specific configuration options for entity ranking
entity_ranking:
  # Splits used to filter for filtered metrics. The split using for evaluation
  # (as set above) will be added automatically if not present.
  filter_splits: [ 'train', 'valid' ]

  # Whether test data should be used for filtering even if the current filter
  # splits do not contain it (most notably: during validation). When this is set
  # to True and "test" is not already a filter split, *additionally* produces
  # "filtered_with_test" metrics (such as MRR or HITS@k). Apparently, many
  # existing models have been trained with this set to True during model
  # selection and using a metric such as
  # mean_reciprocal_rank_filtered_with_test.
  filter_with_test: True

  # How to handle cases with ties between the correct answer and other answers, e.g.,
  #  Query: (s, p, ?).
  #  Answers and score: a:10, b:10, c:10, d:11, e:9
  #  Correct: 'a'.
  #
  # Possible options are:
  # - worst_rank:        Use the highest rank of all answers that have the same
  #                      score as the correct answer. In example: 4.
  # - best_rank:         Use the lowest rank of all answers that have the same
  #                      score as the correct answer (competition scoring). In
  #                      example: 2.
  #                      DO NOT USE THIS OPTION, it leads to misleading evaluation
  #                      results. See https://arxiv.org/pdf/1911.03903.pdf
  # - rounded_mean_rank: Average between worst and best rank, rounded up
  #                      (rounded fractional ranking). In example: 3.
  tie_handling: rounded_mean_rank

  # Compute Hits@K for these choices of K
  hits_at_k_s: [1, 3, 10, 50, 100, 200, 300, 400, 500, 1000]

  # Perform evaluation in chunks of the specified size. When set, score against
  # at most this many entities simultaneouly during prediction. This reduces
  # memory consumption but may increase runtime. Useful when there are many
  # entities and/or memory-intensive models are used.
  chunk_size: -1                  # default: no chunking

  # Metrics are always computed over the entire evaluation data. Optionally,
  # certain more specific metrics can be computed in addition.
  metrics_per:
    head_and_tail: False          # head, tail; also applied to relation_type below
    relation_type: False          # 1-to-1, 1-to-N, N-to-1, N-to-N
    argument_frequency: False     # 25%, 50%, 75%, top quantiles per argument



## EVALUATION ##################################################################


## HYPERPARAMETER SEARCH #######################################################

# Options of hyperparameter search jobs (job.type=="search").
search:
  # The type of search to run (see descriptions below). Possible values: manual,
  # grid, ax
  type: ax

  # Maximum number of parallel training jobs to run during a search.
  num_workers: 1

  # Device pool to use for training jobs. If this list is empty, `job.device` is
  # used for all parallel searches. Otherwise, the first `search.num_workers`
  # devices from this list are used. If the number of devices specified here is
  # less than `search.num_workers`, the list wraps around so that devices are
  # used by multiple jobs in parallel.
  device_pool: [ ]

  # What to do when an error occurs during a training job. Possible values:
  # continue, abort
  on_error: abort

# Manually specify all configurations to try
manual_search:
  # If false, only creates training job folders but does not run the jobs.
  run: True

  # List of configurations to search. Each entry is a record with a field
  # 'folder' (where the training job is stored) and an arbitrary number of other
  # fields that define the search configuration (e.g.
  # 'train.optimizer_args.lr').
  configurations: []


# Metajob for a grid search. Creates a manual search job with all points on the
# grid.
grid_search:
  # If false, only creates manual search job configuration file but does not run
  # it. This may be useful to edit the configuration (e.g., change folder names)
  # or to copy the created configurations to an existing manual search job.
  run: True

  # Define the grid. This is a dict where the key is a (flattened or nested)
  # configuration option (e.g., "train.optimizer_args.lr") and the value is an
  # array of grid-search values (e.g., [ 0.1, 0.01 ]). No default values
  # specified.
  parameters:
    +++: +++

# Dynamic search job that picks configurations using ax
ax_search:
  # Total number of trials to run. Can be increased when a search job is
  # resumed.
  num_trials: 10

  # Number of sobol trials to run (-1: automatic); remaining trials are GP+EI.
  # If equal or larger than num_trials, only Sobal trials will be run.
  num_sobol_trials: -1

  # Random seed for generating the sobol sequence. Has to be fixed for each
  # experiment, or else resuming the sobol sequence is inconsistent.
  sobol_seed: 0

  # Search space definition passed to ax. See create_experiment in
  # https://ax.dev/api/service.html#module-ax.service.ax_client
  parameters: []
  parameter_constraints: []


## USER PARAMETERS #####################################################################

# These parameters are not used by the kge framework itself. It can be used to
# add additional configuration options or information to this config.
user:
  +++: +++
